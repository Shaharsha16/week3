# -*- coding: utf-8 -*-
"""internship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TIGgMNDvst5MCLRRJDkK34iXvHWSL8do
"""

from google.colab import drive
drive.mount('/my-drive')

import zipfile

zip_path = "Tree_Species_Dataset.zip"  # Use exact name shown after upload
extract_path = "Tree_Species_Dataset"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Dataset extracted to:", extract_path)

import os

print("Folders in dataset:")
print(os.listdir(extract_path))

!ls /my-drive/MyDrive/*.zip

import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

dataset_path = "/content/dataset/Tree_Species_Dataset"
labels = sorted(os.listdir(dataset_path))
print("Number of Classes:", len(labels))
print("Class Names:", labels)

# Show 1 image from each of the first 3 classes
for label in labels[:3]:
    class_path = os.path.join(dataset_path, label)
    img_list = os.listdir(class_path)
    if img_list:
        img_path = os.path.join(class_path, img_list[0])
        img = mpimg.imread(img_path)
        plt.imshow(img)
        plt.title(label)
        plt.axis('off')
        plt.show()

import os
import matplotlib.pyplot as plt
from PIL import Image

# Set your dataset path
repo_path = "/content/Tree_Species_Dataset"
class_dirs = sorted(os.listdir(repo_path))

def show_sample_images(repo_path, class_dirs, n=5):
    plt.figure(figsize=(15, 10))
    for i, class_dir in enumerate(class_dirs[:n]):
        img_folder = os.path.join(repo_path, class_dir)
        img_files = os.listdir(img_folder)


        if len(img_files) == 0:
            continue

        img_path = os.path.join(img_folder, img_files[0])
        img = Image.open(img_path)

        plt.subplot(1, n, i + 1)
        plt.imshow(img)
        plt.title(class_dir)
        plt.axis('off')

    plt.show()


show_sample_images(repo_path, class_dirs)

import os
import hashlib
from collections import defaultdict
from PIL import Image

# Define dataset path
repo_path = "/content/Tree_Species_Dataset"
class_dirs = sorted(os.listdir(repo_path))


#  Check for duplicate images

hashes = defaultdict(list)

for class_dir in class_dirs:
    class_path = os.path.join(repo_path, class_dir)
    for img_file in os.listdir(class_path):
        img_path = os.path.join(class_path, img_file)
        try:
            with open(img_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
            hashes[file_hash].append(img_path)
        except Exception as e:
            print(f"Error reading {img_path}: {e}")

duplicates = {h: files for h, files in hashes.items() if len(files) > 1}
print("‚úÖ Duplicate image sets found:", len(duplicates))



#  Check for corrupted images

corrupt_images = []

for class_dir in class_dirs:
    class_path = os.path.join(repo_path, class_dir)
    for img_file in os.listdir(class_path):
        img_path = os.path.join(class_path, img_file)
        try:
            img = Image.open(img_path)
            img.verify()  # Will raise if image is corrupt
        except Exception as e:
            corrupt_images.append(img_path)

print("‚úÖ Corrupted images found:", len(corrupt_images))

import matplotlib.pyplot as plt
from PIL import Image
import os


def show_duplicate_sets(duplicates, sets_to_display=3):
    shown = 0
    for hash_val, dup_paths in duplicates.items():
        if shown >= sets_to_display:
            break

        print(f"üîÅ Duplicate set {shown + 1}:\n")
        plt.figure(figsize=(15, 4))

        for i, img_path in enumerate(dup_paths):
            try:
                img = Image.open(img_path)

                plt.subplot(1, len(dup_paths), i + 1)
                plt.imshow(img)
                plt.title(os.path.basename(img_path))
                plt.axis('off')

            except Exception as e:
                print(f"Error reading {img_path}: {e}")
                continue

        plt.tight_layout()
        plt.show()
        shown += 1

show_duplicate_sets(duplicates, sets_to_display=3)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

img_size = (224, 224)
batch_size = 32

train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    dataset_path,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

val_generator = train_datagen.flow_from_directory(
    dataset_path,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(labels), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(
    train_generator,
    epochs=5,
    validation_data=val_generator
)

# Save the trained model
model.save("basic_cnn_tree_species.h5")

from google.colab import files
files.download("basic_cnn_tree_species.h5")

import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.callbacks import EarlyStopping

# Path to your dataset
dataset_path = "/content/dataset/Tree_Species_Dataset"
img_size = (224, 224)
batch_size = 32

# View classes
labels = sorted(os.listdir(dataset_path))
print("Classes:", labels)

# Setup train/val generators
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    dataset_path,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

val_generator = train_datagen.flow_from_directory(
    dataset_path,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

# Build CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(labels), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(
    train_generator,
    epochs=5,
    validation_data=val_generator,
    callbacks=[EarlyStopping(patience=2)]
)

# Save the model as .h5
model.save("basic_cnn_tree_species.h5")
print("‚úÖ Model saved as basic_cnn_tree_species.h5")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Load training and validation datasets
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    '/content/Tree_Species_Dataset',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_generator = train_datagen.flow_from_directory(
    '/content/Tree_Species_Dataset',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

# Step 1: Import required libraries
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Step 2: Set up image data generators
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    '/content/Tree_Species_Dataset',  # <-- Update this path if needed
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_generator = train_datagen.flow_from_directory(
    '/content/Tree_Species_Dataset',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# Step 3: Load EfficientNetB0 without top layers
base_model = EfficientNetB0(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)
base_model.trainable = False  # Freeze base model

# Step 4: Add custom classification head
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
output = Dense(train_generator.num_classes, activation='softmax')(x)

# Step 5: Build and compile the model
model = Model(inputs=base_model.input, outputs=output)

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Step 6: Train the model
model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=5
)

# Step 7: Save the model in modern format
model.save('efficientnet_tree_species.keras')  # preferred over .h5

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

model_bn = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

model_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model_bn.fit(train_generator, validation_data=val_generator, epochs=5)

model_bn.save('cnn_batchnorm_tree_species.h5')